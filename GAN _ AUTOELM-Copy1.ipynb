{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "#from optparse import OptionParser\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import random\n",
    "import pandas as pd\n",
    "from numpy.linalg import pinv\n",
    "from numpy.linalg import inv\n",
    "random.seed(0)\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hyper-parameters for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numLags = 100\n",
    "predictionStep = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>data</th>\n",
       "      <th>timeofday</th>\n",
       "      <th>dayofweek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-07-01 00:00:00</td>\n",
       "      <td>10844</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-07-01 00:30:00</td>\n",
       "      <td>8127</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-07-01 01:00:00</td>\n",
       "      <td>6210</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-07-01 01:30:00</td>\n",
       "      <td>4656</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-07-01 02:00:00</td>\n",
       "      <td>3820</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time   data  timeofday  dayofweek\n",
       "0  2014-07-01 00:00:00  10844          0          1\n",
       "1  2014-07-01 00:30:00   8127         30          1\n",
       "2  2014-07-01 01:00:00   6210         60          1\n",
       "3  2014-07-01 01:30:00   4656         90          1\n",
       "4  2014-07-01 02:00:00   3820        120          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet = '../nyc_taxi'\n",
    "filePath = dataSet+'.csv'\n",
    "df = pd.read_csv(filePath, header=0, skiprows=[1,2], names=['time', 'data', 'timeofday', 'dayofweek'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17520 entries, 0 to 17519\n",
      "Data columns (total 4 columns):\n",
      "time         17520 non-null object\n",
      "data         17520 non-null int64\n",
      "timeofday    17520 non-null int64\n",
      "dayofweek    17520 non-null int64\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 547.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time         0\n",
       "data         0\n",
       "timeofday    0\n",
       "dayofweek    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # standardize data by subtracting mean and dividing by std\n",
    "  meanSeq = np.mean(df['data'])\n",
    "  stdSeq = np.std(df['data'])\n",
    "  df['data'] = (df['data'] - meanSeq)/stdSeq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input-target pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate time embedded matrix\n",
      "input shape:  (17520, 100)\n",
      "target shape:  (17520, 1)\n"
     ]
    }
   ],
   "source": [
    "def getTimeEmbeddedMatrix(sequence, numLags=100, predictionStep=1):\n",
    "  print(\"generate time embedded matrix\")\n",
    "  inDim = numLags\n",
    "  X = np.zeros(shape=(len(sequence), inDim))\n",
    "  T = np.zeros(shape=(len(sequence), 1))\n",
    "  for i in range(numLags-1, len(sequence)-predictionStep):\n",
    "    X[i, :] = np.array(sequence['data'][(i-numLags+1):(i+1)])\n",
    "    T[i, :] = sequence['data'][i+predictionStep]\n",
    "  print('input shape: ',X.shape)\n",
    "  print('target shape: ',T.shape)\n",
    "  return (X, T)\n",
    "\n",
    "(X, T) = getTimeEmbeddedMatrix(df, numLags, predictionStep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17520, 100) (17520, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOELM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: (25, 100)\n",
      "bias: (1, 25)\n",
      "input features: (1, 100)\n",
      "[[26.73015071 27.98622253 29.20842738 25.69226713 29.06920576 25.30993178\n",
      "  24.07201064 24.31074231 28.16047585 27.52178154 27.86130065 28.58902652\n",
      "  27.49881974 29.0730478  26.64207791 29.25626895 28.32753515 24.11616073\n",
      "  27.50254857 25.18363982 24.77184242 26.32346954 28.48496744 27.26313313\n",
      "  25.02408901]]\n",
      "hidden (before nonlinear activation): (1, 25)\n",
      "hidden (after nonlinear activateion): (1, 25)\n",
      "hidden (after nonlinear activateion): (1, 25)\n"
     ]
    }
   ],
   "source": [
    "weights = np.random.random((25,100))\n",
    "print('weights:',weights.shape)\n",
    "\n",
    "bias = np.random.random((1,25)) * 2 -1\n",
    "print('bias:', bias.shape)\n",
    "\n",
    "features = np.random.random((1,100))\n",
    "print('input features:', features.shape)\n",
    "\n",
    "def linear(features,weights,bias):\n",
    "    print(np.dot(features, np.transpose(weights))  + bias)\n",
    "    return np.dot(features, np.transpose(weights)) #+ bias\n",
    "\n",
    "hidden = linear(features,weights,bias)\n",
    "print('hidden (before nonlinear activation):', hidden.shape)\n",
    "\n",
    "def sigmoidActFunc(features):\n",
    "  return 1.0 / (1.0 + np.exp(-features))\n",
    "\n",
    "hidden = sigmoidActFunc(hidden)\n",
    "print('hidden (after nonlinear activateion):', hidden.shape)\n",
    "\n",
    "def reluActFunc(features):\n",
    "  return np.maximum(0,features)\n",
    "hidden = reluActFunc(hidden)\n",
    "print('hidden (after nonlinear activateion):', hidden.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nDimInput = 100\n",
    "nDimOutput = 100\n",
    "numNeurons = 25\n",
    "lamb=0.0001\n",
    "outputWeightFF = 0.92 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total initial:  249975\n",
      "total sequential:  0\n"
     ]
    }
   ],
   "source": [
    "# comment these two lines when you want to alter things\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, T, test_size=0.2)\n",
    "border = int(9999 * numNeurons)\n",
    "x_train_init = X_train[:border] # it was 6250 before\n",
    "x_train_seq = X_train[border:] # was 7766 before\n",
    "\n",
    "print('total initial: ', (border))\n",
    "print('total sequential: ', len(x_train_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTOELM implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "class AUTOELM(object):\n",
    "\n",
    "    def __init__(self, inputs, outputs, numHiddenNeurons, forgettingFactor=0.999):\n",
    "        self.name = 'AUTOELM'\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.numHiddenNeurons = numHiddenNeurons\n",
    "\n",
    "        # input to hidden weights\n",
    "        self.inputWeights = None\n",
    "        # bias of hidden units\n",
    "        self.bias = None\n",
    "        # hidden to output layer connection\n",
    "        self.beta = None\n",
    "        # auxiliary matrix used for sequential learning\n",
    "        self.M = None\n",
    "\n",
    "        self.forgettingFactor = forgettingFactor\n",
    "        self.bf= None\n",
    "        self.k = None\n",
    "        self.mask = None\n",
    "\n",
    "    def calculateHiddenLayerActivation(self, features):\n",
    "        print('features',features.shape,'wieghts',self.inputWeights,'bias',self.bias)\n",
    "        V = (np.dot(features, np.transpose(self.inputWeights))  + self.bias)\n",
    "        #H = sigmoidActFunc(V)\n",
    "        print(V)\n",
    "        H = reluActFunc(V)\n",
    "        print(H.shape,H)\n",
    "        return H\n",
    "\n",
    "\n",
    "    def initializePhase(self, lamb=0.0001):\n",
    "        \"\"\"\n",
    "        Step 1: Initialization phase\n",
    "        \"\"\"\n",
    "        # randomly initialize the input->hidden connections\n",
    "        self.inputWeights = np.random.random((self.inputs, self.numHiddenNeurons))\n",
    "        self.bf = np.random.random()\n",
    "        self.inputWeights = self.inputWeights * 2 - 1\n",
    "        self.bias = np.random.random((self.numHiddenNeurons)) * 2 - 1\n",
    "        # auxiliary matrix used for sequential learning\n",
    "        self.M = inv(lamb*np.eye(self.numHiddenNeurons))\n",
    "        # hidden to output layer connection\n",
    "        self.beta = np.zeros([self.numHiddenNeurons,self.outputs])\n",
    "        self.k = np.zeros([self.numHiddenNeurons,self.numHiddenNeurons])\n",
    "    \n",
    "    def init_training(self , features, targets):\n",
    "        H = np.dot(self.inputs, self.inputWeights) + self.bias\n",
    "        H = 1/(1+np.exp(-H)) # sigmoid activation function\n",
    "        HT = np.transpose(H)\n",
    "        I = np.identity(self.numHiddenNeurons)\n",
    "        C_I = I\n",
    "        HTH = np.dot(HT, H)\n",
    "        K = C_I + HTH\n",
    "        self.k = K\n",
    "        inverse_k = np.linalg.inv(K)\n",
    "        H_inverse = np.dot(inverse_k,HT)\n",
    "        sin_y = np.sin(self.outputs)\n",
    "        arcsin_y = np.arcsin(sin_y)\n",
    "        inverse_acti_y = arcsin_y\n",
    "        An = np.dot(H_inverse, inverse_acti_y)\n",
    "        AnHf = np.dot(H,An)\n",
    "        #Bn = mean_squared_error(AnHf,inverse_acti_y)\n",
    "        Bn = np.square(np.subtract(AnHf,inverse_acti_y)).mean() \n",
    "        Bn = np.sqrt(Bn)\n",
    "        self.beta = An\n",
    "        init_train = Bn\n",
    "        print(\"init training complete!\")\n",
    "        return init_train\n",
    "    \n",
    "    def seq_train(self, features, targets, mask):\n",
    "        self.inputWeights = np.transpose(self.beta)  # assigns beta weights to alpha \n",
    "        H = np.dot(features, self.inputWeights) + self.bf\n",
    "        H = 1/(1+np.exp(-H))\n",
    "        HT = np.transpose(H)\n",
    "        HTH = np.dot(HT, H)\n",
    "        K = self.k + HTH\n",
    "        self.k = K\n",
    "        #addition\n",
    "        K_dirty = K + 0.00001*np.random.rand(25, 25)\n",
    "        # * #\n",
    "        K_inverse = np.linalg.inv(K_dirty)\n",
    "        sin_y = np.sin(self.outputs)\n",
    "        arcsin_y = np.arcsin(sin_y)\n",
    "        inverse_acti_y = arcsin_y\n",
    "        UPDATE = np.dot(np.dot(K_inverse, HT), inverse_acti_y - np.dot(H, self.beta))  # [D x m]    \n",
    "        ###############################multiplication with mask  makes it zero ############3\n",
    "        #UPDATE = np.dot(mask, UPDATE)\n",
    "        ###############################multiplication with mask  makes it zero ############3\n",
    "        An = self.beta + UPDATE\n",
    "        self.beta = An\n",
    "        AnHf = np.dot(H, An)\n",
    "        Bn = np.square(np.subtract(AnHf,inverse_acti_y)).mean() \n",
    "        Bn = np.sqrt(Bn)\n",
    "        self.bf = Bn\n",
    "        seq_train =  Bn\n",
    "        return seq_train\n",
    "    \n",
    "    def retrieve_beta(self):\n",
    "        return self.beta\n",
    "    \n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        Make prediction with feature matrix\n",
    "        :param features: feature matrix with dimension (numSamples, numInputs)\n",
    "        :return: predictions with dimension (numSamples, numOutputs)\n",
    "        \"\"\"\n",
    "       # tf.matmul(self.__activation(tf.matmul(self.__x, self.inputWeights) + self.__bf), self.__beta)\n",
    "        H = (np.dot(features,self.inputWeights) + self.bf)\n",
    "        H = 1/(1+np.exp(-H))\n",
    "        H = np.dot(H,self.beta)\n",
    "        prediction = H\n",
    "        return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perform_sequential_training(train_set, auto_elm, epochs, x_seq_train, batch_size, h_nodes, number_neurons, flag):\n",
    "    neuron = random.randint(0, h_nodes - 1) # select initial hidden neuron\n",
    "    mask = np.zeros([h_nodes, h_nodes]) # mask of hidden neuron dimensions\n",
    "    mask[neuron,neuron] = 1 # add a single digit to a row (corresponds to single neuron)\n",
    "        # mask = np.eye(256) # update all nodes\n",
    "    group = 1 # group of images\n",
    "    count = train_set # number of training images\n",
    "\n",
    "        # actual network training phase\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(x_seq_train), batch_size):\n",
    "            x_batch = x_seq_train[i:i+batch_size]\n",
    "            auto_elm.seq_train(x_batch, x_batch, mask)\n",
    "        \n",
    "        # fetch the node beta weight data\n",
    "        node = auto_elm.retrieve_beta()[neuron]\n",
    "    \n",
    "    print(\"Neuron Used: {}\\n\".format(neuron))\n",
    "    print(\"sequential training complete!\")\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# added for beta weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init training complete!\n",
      "Neuron Used: 13\n",
      "\n",
      "sequential training complete!\n",
      "augmented data X shape (1411200, 100) (14400, 100)\n",
      "augmented data Y shape (305760, 1) (3120, 1)\n"
     ]
    }
   ],
   "source": [
    "auto_elm = AUTOELM(inputs=nDimInput,outputs=nDimOutput, numHiddenNeurons=numNeurons,forgettingFactor=outputWeightFF)\n",
    "auto_elm.initializePhase(lamb=lamb)\n",
    "encoding_0 = auto_elm.init_training(x_train_init,x_train_init)\n",
    "decoding_0 = perform_sequential_training(10,auto_elm,20,x_train_seq,50,numNeurons,0,False)\n",
    "\n",
    "# Block added here for using Beta values to generate more training data\n",
    "thr = 1200 * 12  # NB_YEARS * 12 (months)\n",
    "\n",
    "x_train_temp1 = X[:thr]\n",
    "y_train_temp1 = T[thr:]\n",
    "\n",
    "x_test1 = X[:thr]\n",
    "y_test1 = T[thr:]\n",
    "\n",
    "\n",
    "nb_copies = 98\n",
    "X_train_fake = np.concatenate([\n",
    "    x_train_temp1\n",
    "    + 5 * decoding_0 - 5  # global offset with a (possibly large) scalar\n",
    "    + 2 * np.random.random(x_train_temp1.shape) - 1  # small element-wise offset\n",
    "    for _ in range(nb_copies)\n",
    "])\n",
    "y_train_fake = np.concatenate([\n",
    "    y_train_temp1\n",
    "    for _ in range(nb_copies)\n",
    "])\n",
    "\n",
    "\n",
    "print('augmented data X shape',X_train_fake.shape, x_test1.shape)\n",
    "print('augmented data Y shape',y_train_fake.shape, y_test1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1411200, 100) (305760, 1)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "print(X_train_fake.shape,y_train_fake.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.scatter(X_train_fake[:,50],X_train_fake[:,99])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.33858799, -4.95754826, -5.80792702, ..., -5.3130586 ,\n",
       "        -5.36585897, -4.06029657],\n",
       "       [-5.79693473, -4.40958196, -5.16049077, ..., -5.19759485,\n",
       "        -5.84507687, -5.74970345],\n",
       "       [-4.2470216 , -5.90622475, -5.09618882, ..., -5.79187107,\n",
       "        -5.19543995, -5.59907365],\n",
       "       ...,\n",
       "       [-2.88833576, -4.44770961, -3.34010372, ..., -5.85101442,\n",
       "        -5.15106297, -6.00666212],\n",
       "       [-3.78297217, -2.63196928, -3.35640777, ..., -5.52798815,\n",
       "        -5.99770408, -5.70987868],\n",
       "       [-3.89675758, -4.13294899, -3.90765896, ..., -4.6107011 ,\n",
       "        -4.63416202, -5.04385437]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14016, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5362914891867696"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxElement = np.amax(X_train)\n",
    "maxElement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import hstack\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim = 2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(latent_dim, outputs=2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "    model.add(Dense(outputs, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss3\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14016, 100)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(n = 1401600):\n",
    "    X1 = (X_train - 4) * -0.5\n",
    "    X2 = np.sqrt(X1)\n",
    "    X1 = X1.reshape(n, 1)\n",
    "    X2 = X2.reshape(n, 1)\n",
    "    X = hstack((X1, X2))\n",
    "    # generate class labels\n",
    "    y = ones((n, 1))\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1,y1 = generate_real_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.33858799, -4.95754826, -5.80792702, ..., -5.3130586 ,\n",
       "        -5.36585897, -4.06029657],\n",
       "       [-5.79693473, -4.40958196, -5.16049077, ..., -5.19759485,\n",
       "        -5.84507687, -5.74970345],\n",
       "       [-4.2470216 , -5.90622475, -5.09618882, ..., -5.79187107,\n",
       "        -5.19543995, -5.59907365],\n",
       "       ...,\n",
       "       [-2.88833576, -4.44770961, -3.34010372, ..., -5.85101442,\n",
       "        -5.15106297, -6.00666212],\n",
       "       [-3.78297217, -2.63196928, -3.35640777, ..., -5.52798815,\n",
       "        -5.99770408, -5.70987868],\n",
       "       [-3.89675758, -4.13294899, -3.90765896, ..., -4.6107011 ,\n",
       "        -4.63416202, -5.04385437]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n):\n",
    "    #x_input = randn(len(X_train))\n",
    "    # generate points in the latent space\n",
    "    x_input = X_train_fake[:1401600]\n",
    "    #x_input = x_input.reshape(n, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n)\n",
    "    # predict outputs\n",
    "    X = generator.predict(x_input)\n",
    "    # create class labels\n",
    "    y = zeros((len(X), 1))\n",
    "    return X, y\n",
    "\n",
    "# evaluate the discriminator and plot real and fake points\n",
    "def summarize_performance(epoch, generator, discriminator, latent_dim, n=1401600 ):\n",
    "    # prepare real samples\n",
    "    x_real, y_real = generate_real_samples(n)\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n",
    "    # prepare fake examples\n",
    "    x_fake, y_fake = generate_fake_samples(generator, latent_dim, n)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    print(epoch, acc_real, acc_fake)\n",
    "    # scatter plot real and fake data points\n",
    "    pyplot.scatter(x_real[:, 0], x_real[:, 1], color='red')\n",
    "    pyplot.scatter(x_fake[:, 0], x_fake[:, 1], color='blue')\n",
    "    pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5000\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# We will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.74576409 1.32127366]\n",
      " [1.91553193 1.38402743]\n",
      " [2.00459407 1.41583688]\n",
      " ...\n",
      " [2.15889515 1.46931792]\n",
      " [2.31033745 1.51997942]\n",
      " [2.42718111 1.55794131]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-c24f019cbe31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_real\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# prepare fake examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mx_fake\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_fake_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhalf_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m#gen_loss = generator_loss(x_fake)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-4cc6e8f0445b>\u001b[0m in \u001b[0;36mgenerate_fake_samples\u001b[1;34m(generator, latent_dim, n)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mx_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_latent_points\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# predict outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;31m# create class labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "n_batch=1401600 \n",
    "n_epochs = 5000\n",
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "#with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "for i in range(n_epochs):\n",
    "    half_batch = int(n_batch)\n",
    "    x_real, y_real = generate_real_samples(half_batch)\n",
    "    print(x_real)\n",
    "    # prepare fake examples\n",
    "    x_fake, y_fake = generate_fake_samples(generator, latent_dim, half_batch)\n",
    "    print(x_fake)\n",
    "        #gen_loss = generator_loss(x_fake)\n",
    "        #disc_loss = discriminator_loss(x_real, x_fake)\n",
    "\n",
    "        #gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        #gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "        #generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "        #discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, latent_dim, n_epochs=5000, n_batch=1401600 , n_eval=100):\n",
    "    # determine half the size of one batch, for updating the discriminator\n",
    "    half_batch = int(n_batch)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # prepare real samples\n",
    "        x_real, y_real = generate_real_samples(half_batch)\n",
    "        # prepare fake examples\n",
    "        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "        # update discriminator\n",
    "        d_model.train_on_batch(x_real, y_real)\n",
    "        d_model.train_on_batch(x_fake, y_fake)\n",
    "        # prepare points in latent space as input for the generator\n",
    "        x_gan = generate_latent_points(latent_dim, n_batch)\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = ones((n_batch, 1))\n",
    "        # update the generator via the discriminator's error\n",
    "        gan_model.train_on_batch(x_gan, y_gan)\n",
    "        # evaluate the model every n_eval epochs\n",
    "        if (i+1) % n_eval == 0:\n",
    "            summarize_performance(i, g_model, d_model, latent_dim)\n",
    "\n",
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# create the discriminator\n",
    "discriminator = discriminator()\n",
    "# create the generator\n",
    "generator = generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = GAN(generator, discriminator)\n",
    "# train model\n",
    "from datetime import datetime\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"training started\" ,dt_string)\n",
    "train(generator, discriminator, gan_model, latent_dim)\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"training ended\" ,dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
